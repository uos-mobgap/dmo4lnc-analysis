{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf365017-4f09-4af5-90ab-37b3aa64ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.signal import butter, filtfilt, correlate, find_peaks\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.linalg import logm\n",
    "from scipy.io import savemat, loadmat\n",
    "import scipy.io as spio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from vedo import Points, Plotter, Line, Axes, Plane, Grid\n",
    "from IPython.display import Video\n",
    "import imageio.v2 as imageio\n",
    "import shutil\n",
    "from pingouin import intraclass_corr\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mobgap.data import GenericMobilisedDataset\n",
    "from mobgap.pipeline import MobilisedPipelineImpaired, MobilisedPipelineHealthy\n",
    "from mobgap.utils.misc import get_env_var\n",
    "from mobgap.aggregation import get_mobilised_dmo_thresholds\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b835c8ea-3012-4a51-91b8-b742870f793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_boundaries(file_path):\n",
    "    \"\"\"Return (start, end) row indices for each day in the CSV.\"\"\"\n",
    "    time_col = pd.read_csv(file_path, usecols=[\"Time\"])\n",
    "    time_col[\"Time\"] = pd.to_datetime(time_col[\"Time\"], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "    days = time_col[\"Time\"].dt.date\n",
    "    change_idx = days.ne(days.shift()).to_numpy().nonzero()[0]\n",
    "    change_idx = list(change_idx) + [len(days)]\n",
    "\n",
    "    return [(change_idx[i], change_idx[i + 1]) for i in range(len(change_idx) - 1)]\n",
    "\n",
    "\n",
    "def extract_day(file_path, start, end):\n",
    "    \"\"\"Read only rows between [start, end) from CSV.\"\"\"\n",
    "    nrows = end - start\n",
    "    skip = (j for j in range(1, start + 1))  # generator → memory efficient\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        skiprows=skip,\n",
    "        nrows=nrows,\n",
    "        parse_dates=[\"Time\"],\n",
    "        date_parser=lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"),\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sensor_struct(df, fs, name):\n",
    "    \"\"\"Convert one day's dataframe into MATLAB struct for a sensor.\"\"\"\n",
    "    reformatted_time = (df[\"Time\"].astype(\"int64\") / 1e9).values.reshape(-1, 1)\n",
    "    return {\n",
    "        name: {\n",
    "            \"Fs\": {\"Acc\": fs, \"Gyr\": fs},\n",
    "            \"Acc\": df[[\"Accel-X (g)\", \" Accel-Y (g)\", \" Accel-Z (g)\"]].values,\n",
    "            \"Gyr\": df[[\" Gyro-X (d/s)\", \" Gyro-Y (d/s)\", \" Gyro-Z (d/s)\"]].values,\n",
    "            \"Timestamp\": reformatted_time,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def load_metadata(raw_data_folder):\n",
    "    \"\"\"Read subject metadata once, return as infoForAlgo dict.\"\"\"\n",
    "    metadata = pd.read_excel(\n",
    "        os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(raw_data_folder))))\n",
    "        + r\"\\subject_metadata.xlsx\"\n",
    "    )\n",
    "    subject_id = int(os.path.basename(os.path.dirname(os.path.dirname(raw_data_folder))))\n",
    "    subject = metadata.loc[metadata[\"Subject_ID\"] == subject_id]\n",
    "\n",
    "    return {\n",
    "        \"TimeMeasure1\": {\n",
    "            \"Subject_ID\": subject[\"Subject_ID\"].values[0],\n",
    "            \"Cohort\": subject[\"Cohort\"].values[0],\n",
    "            \"Gender\": subject[\"Gender\"].values[0],\n",
    "            \"Handedness\": subject[\"Handedness\"].values[0],\n",
    "            \"Age\": subject[\"Age\"].values[0],\n",
    "            \"Weight\": subject[\"Weight\"].values[0],\n",
    "            \"Height\": subject[\"Height\"].values[0],\n",
    "            \"SensorHeight\": subject[\"SensorHeight\"].values[0],\n",
    "            \"WalkingAid_01\": subject[\"WalkingAid_01\"].values[0],\n",
    "            \"WalkingAid_Side\": subject[\"WalkingAid_Side\"].values[0],\n",
    "            \"WalkingAid_Description\": subject[\"WalkingAid_Description\"].values[0],\n",
    "            \"SensorType_SU\": subject[\"SensorType_SU\"].values[0],\n",
    "            \"SensorAttachment_SU\": subject[\"SensorAttachment_SU\"].values[0],\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def save_to_day_mats(raw_data_folder, regenerate):\n",
    "    \"\"\"\n",
    "    Split wrist & waist CSVs into per-day data.mat files.\n",
    "    Each file contains both sensors (if present), plus metadata.\n",
    "    Skips days where data.mat already exists.\n",
    "    \"\"\"\n",
    "\n",
    "    flag_file_path = os.path.join(raw_data_folder, \"ALL_DAYS_DONE_FLAG\")\n",
    "    \n",
    "    if regenerate:\n",
    "        if os.path.exists(flag_file_path):\n",
    "          os.remove(flag_file_path)\n",
    "    \n",
    "    if not os.path.isfile(flag_file_path):\n",
    "        wrist_file = os.path.join(raw_data_folder, \"wrist.resampled.csv\")\n",
    "        waist_file = os.path.join(raw_data_folder, \"lower_back.resampled.csv\")\n",
    "    \n",
    "        wrist_bounds = get_day_boundaries(wrist_file) if os.path.exists(wrist_file) else []\n",
    "        waist_bounds = get_day_boundaries(waist_file) if os.path.exists(waist_file) else []\n",
    "    \n",
    "        n_days = max(len(wrist_bounds), len(waist_bounds))\n",
    "        if prints:\n",
    "            print(f\"Detected {len(wrist_bounds)} wrist days, {len(waist_bounds)} waist days\")\n",
    "            print(f\"Total output days: {n_days}\")\n",
    "    \n",
    "        infoForAlgo = load_metadata(raw_data_folder)\n",
    "    \n",
    "        for day_idx in range(n_days):\n",
    "            day_folder = os.path.join(raw_data_folder, f\"Day {day_idx + 1}\")\n",
    "            data_path = os.path.join(day_folder, \"data.mat\")\n",
    "    \n",
    "            # Skip if already exists\n",
    "            if os.path.exists(data_path) and not(regenerate):\n",
    "                print(f\"⏩ Skipping Day {day_idx+1}, already exists.\")\n",
    "                continue\n",
    "    \n",
    "            sensors = {}\n",
    "    \n",
    "            # Wrist for this day (if available)\n",
    "            if day_idx < len(wrist_bounds):\n",
    "                start, end = wrist_bounds[day_idx]\n",
    "                df = extract_day(wrist_file, start, end)\n",
    "                fs = float(round(len(df) / (df[\"Time\"].iloc[-1] - df[\"Time\"].iloc[0]).total_seconds()))\n",
    "                sensors.update(build_sensor_struct(df, fs, \"Wrist\"))\n",
    "    \n",
    "            # Waist for this day (if available)\n",
    "            if day_idx < len(waist_bounds):\n",
    "                start, end = waist_bounds[day_idx]\n",
    "                df = extract_day(waist_file, start, end)\n",
    "                fs = float(round(len(df) / (df[\"Time\"].iloc[-1] - df[\"Time\"].iloc[0]).total_seconds()))\n",
    "                sensors.update(build_sensor_struct(df, fs, \"LowerBack\"))\n",
    "    \n",
    "            # Skip empty days\n",
    "            if not sensors:\n",
    "                continue\n",
    "    \n",
    "            # Pick earliest timestamp across sensors for StartDateTime - should be midnight for both so not super important if there's a small difference\n",
    "            all_times = [\n",
    "                s[\"Timestamp\"][0][0] for s in sensors.values() if len(s[\"Timestamp\"]) > 0\n",
    "            ]\n",
    "            start_time = datetime.fromtimestamp(min(all_times)).strftime(\"%d-%b-%Y %H:%M:%S\")\n",
    "    \n",
    "            data = {\n",
    "                \"TimeMeasure1\": {\n",
    "                    \"Test1\": {\n",
    "                        \"Trial1\": {\n",
    "                            \"SU\": sensors,\n",
    "                            \"StartDateTime\": start_time,\n",
    "                            \"TimeZone\": \"Europe/UK\",\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    \n",
    "            os.makedirs(day_folder, exist_ok=True)\n",
    "    \n",
    "            # Save combined .mat files\n",
    "            savemat(data_path, {\"data\": data})\n",
    "            savemat(os.path.join(day_folder, \"infoForAlgo.mat\"), {\"infoForAlgo\": infoForAlgo})\n",
    "    \n",
    "            print(f\"✅ Saved Day {day_idx+1} with sensors: {list(sensors.keys())}\")\n",
    "    \n",
    "        # create blank file as flag that all days are done\n",
    "        with open(flag_file_path, \"w\") as file:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3a3511c-f6c2-4e74-aadd-4a6b4424bc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ac4jmi\\Desktop\\DMO4LNC\\Data Collection\\Dataset\\CP\\383\\Home\\\n",
      "C:\\Users\\ac4jmi\\Desktop\\DMO4LNC\\Data Collection\\Dataset\\CP\\582\\Home\\\n",
      "C:\\Users\\ac4jmi\\Desktop\\DMO4LNC\\Data Collection\\Dataset\\CP\\718\\Home\\\n",
      "C:\\Users\\ac4jmi\\Desktop\\DMO4LNC\\Data Collection\\Dataset\\CP\\855\\Home\\\n",
      "C:\\Users\\ac4jmi\\Desktop\\DMO4LNC\\Data Collection\\Dataset\\CP\\875\\Home\\\n",
      "C:\\Users\\ac4jmi\\Desktop\\DMO4LNC\\Data Collection\\Dataset\\HA\\3\\Home\\\n",
      "C:\\Users\\ac4jmi\\Desktop\\DMO4LNC\\Data Collection\\Dataset\\PSP\\864\\Home\\\n",
      "C:\\Users\\ac4jmi\\Desktop\\DMO4LNC\\Data Collection\\Dataset\\PSP\\899\\Home\\\n"
     ]
    }
   ],
   "source": [
    "prints = False\n",
    "regenerate_data_files = False\n",
    "\n",
    "base_path = r\"C:\\Users\\ac4jmi\\Desktop\\DMO4LNC\\Data Collection\\Dataset\"\n",
    "\n",
    "# Loop through all the participant folders syncing axivity sensors and mocap timings\n",
    "for cohort in os.listdir(base_path):\n",
    "    cohort_path = os.path.join(base_path, cohort)\n",
    "    if os.path.isdir(cohort_path):\n",
    "        for participant in os.listdir(cohort_path):\n",
    "            participant_path = os.path.join(cohort_path, participant)\n",
    "            if os.path.isdir(participant_path):\n",
    "                participant_path = os.path.join(participant_path, 'Home\\\\')\n",
    "                print(participant_path)\n",
    "                save_to_day_mats(participant_path, regenerate = regenerate_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad9545a-359c-410b-840e-eea87cd35b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from .mat files...\n",
      "GenericMobilisedDataset [66 groups/rows]\n",
      "\n",
      "      cohort subject_id location    day   TimeMeasure   Test   Trial\n",
      "   0      CP        383     Home  Day 1  TimeMeasure1  Test1  Trial1\n",
      "   1      CP        383     Home  Day 2  TimeMeasure1  Test1  Trial1\n",
      "   2      CP        383     Home  Day 3  TimeMeasure1  Test1  Trial1\n",
      "   3      CP        383     Home  Day 4  TimeMeasure1  Test1  Trial1\n",
      "   4      CP        383     Home  Day 5  TimeMeasure1  Test1  Trial1\n",
      "   ..    ...        ...      ...    ...           ...    ...     ...\n",
      "   61    PSP        899     Home  Day 5  TimeMeasure1  Test1  Trial1\n",
      "   62    PSP        899     Home  Day 6  TimeMeasure1  Test1  Trial1\n",
      "   63    PSP        899     Home  Day 7  TimeMeasure1  Test1  Trial1\n",
      "   64    PSP        899     Home  Day 8  TimeMeasure1  Test1  Trial1\n",
      "   65    PSP        899     Home  Day 9  TimeMeasure1  Test1  Trial1\n",
      "   \n",
      "   [66 rows x 7 columns]\n",
      "wb_id\n",
      "0      Wednesday\n",
      "1      Wednesday\n",
      "2      Wednesday\n",
      "3      Wednesday\n",
      "4      Wednesday\n",
      "         ...    \n",
      "298    Wednesday\n",
      "299    Wednesday\n",
      "300    Wednesday\n",
      "301    Wednesday\n",
      "302    Wednesday\n",
      "Name: start, Length: 303, dtype: object\n",
      "wb_id\n",
      "0      Thursday\n",
      "1      Thursday\n",
      "2      Thursday\n",
      "3      Thursday\n",
      "4      Thursday\n",
      "         ...   \n",
      "344    Thursday\n",
      "345    Thursday\n",
      "346    Thursday\n",
      "347    Thursday\n",
      "348    Thursday\n",
      "Name: start, Length: 349, dtype: object\n",
      "wb_id\n",
      "0      Friday\n",
      "1      Friday\n",
      "2      Friday\n",
      "3      Friday\n",
      "4      Friday\n",
      "        ...  \n",
      "282    Friday\n",
      "283    Friday\n",
      "284    Friday\n",
      "285    Friday\n",
      "286    Friday\n",
      "Name: start, Length: 287, dtype: object\n",
      "wb_id\n",
      "0      Saturday\n",
      "1      Saturday\n",
      "2      Saturday\n",
      "3      Saturday\n",
      "4      Saturday\n",
      "         ...   \n",
      "232    Saturday\n",
      "233    Saturday\n",
      "234    Saturday\n",
      "235    Saturday\n",
      "236    Saturday\n",
      "Name: start, Length: 237, dtype: object\n",
      "wb_id\n",
      "0      Sunday\n",
      "1      Sunday\n",
      "2      Sunday\n",
      "3      Sunday\n",
      "4      Sunday\n",
      "        ...  \n",
      "174    Sunday\n",
      "175    Sunday\n",
      "176    Sunday\n",
      "177    Sunday\n",
      "178    Sunday\n",
      "Name: start, Length: 179, dtype: object\n",
      "wb_id\n",
      "0      Monday\n",
      "1      Monday\n",
      "2      Monday\n",
      "3      Monday\n",
      "4      Monday\n",
      "        ...  \n",
      "366    Monday\n",
      "367    Monday\n",
      "368    Monday\n",
      "369    Monday\n",
      "370    Monday\n",
      "Name: start, Length: 371, dtype: object\n",
      "wb_id\n",
      "0      Tuesday\n",
      "1      Tuesday\n",
      "2      Tuesday\n",
      "3      Tuesday\n",
      "4      Tuesday\n",
      "        ...   \n",
      "279    Tuesday\n",
      "280    Tuesday\n",
      "281    Tuesday\n",
      "282    Tuesday\n",
      "283    Tuesday\n",
      "Name: start, Length: 284, dtype: object\n",
      "wb_id\n",
      "0      Wednesday\n",
      "1      Wednesday\n",
      "2      Wednesday\n",
      "3      Wednesday\n",
      "4      Wednesday\n",
      "         ...    \n",
      "340    Wednesday\n",
      "341    Wednesday\n",
      "342    Wednesday\n",
      "343    Wednesday\n",
      "344    Wednesday\n",
      "Name: start, Length: 345, dtype: object\n",
      "wb_id\n",
      "0     Thursday\n",
      "1     Thursday\n",
      "2     Thursday\n",
      "3     Thursday\n",
      "4     Thursday\n",
      "        ...   \n",
      "85    Thursday\n",
      "86    Thursday\n",
      "87    Thursday\n",
      "88    Thursday\n",
      "89    Thursday\n",
      "Name: start, Length: 90, dtype: object\n",
      "Day 10 does not exist for subject 855\n",
      "Series([], Name: start, dtype: object)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m pipeline\u001b[38;5;241m.\u001b[39maggregated_parameters_\u001b[38;5;241m.\u001b[39minsert(loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(subject))\n\u001b[0;32m     86\u001b[0m pipeline\u001b[38;5;241m.\u001b[39maggregated_parameters_\u001b[38;5;241m.\u001b[39minsert(loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m, value \u001b[38;5;241m=\u001b[39m day)\n\u001b[1;32m---> 87\u001b[0m pipeline\u001b[38;5;241m.\u001b[39maggregated_parameters_\u001b[38;5;241m.\u001b[39minsert(loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, value \u001b[38;5;241m=\u001b[39m day_wb_started\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     88\u001b[0m pipeline\u001b[38;5;241m.\u001b[39maggregated_parameters_\u001b[38;5;241m.\u001b[39minsert(loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcohort\u001b[39m\u001b[38;5;124m\"\u001b[39m, value \u001b[38;5;241m=\u001b[39m cohort)\n\u001b[0;32m     89\u001b[0m pipeline\u001b[38;5;241m.\u001b[39maggregated_parameters_\u001b[38;5;241m.\u001b[39minsert(loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m, value \u001b[38;5;241m=\u001b[39m meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "def get_paths_with_extension(extension, dataset_folder_name=\"Dataset\"):\n",
    "    paths = []\n",
    "    dataset_folder = os.path.join(os.getcwd(), dataset_folder_name)\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_folder):\n",
    "        # Skip any folder path containing \"\\Lab\\\"\n",
    "        if \"Lab\" in root.split(os.sep):\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith(extension):\n",
    "                path = os.path.join(root, file)\n",
    "                paths.append(path)\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def get_mobilised_dataset():\n",
    "\n",
    "    print(\"Building dataset from .mat files...\")\n",
    "    paths_list = get_paths_with_extension(\"data.mat\")\n",
    "    # Build a dataset from the .mat files\n",
    "    dataset = GenericMobilisedDataset(\n",
    "        paths_list = paths_list,\n",
    "        test_level_names = [\"TimeMeasure\", \"Test\", \"Trial\"], # This is the structure (without numbers) of the data inside the data.mat struct.\n",
    "        measurement_condition = \"laboratory\", # I'm not currently sure what this does on the algorithm side of things, can also be \"free_living\".\n",
    "        parent_folders_as_metadata=[\"cohort\", \"subject_id\", \"location\", \"day\"] # these are the column headings for the metadata, the columns get populated with the folder names.\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# get mobgap dataset\n",
    "dataset = get_mobilised_dataset()\n",
    "print(dataset)\n",
    "subject_metadata = pd.read_excel(\"Dataset/subject_metadata.xlsx\")\n",
    "\n",
    "cohorts = [\"CP\", \"PSP\"]\n",
    "\n",
    "all_pipeline_day_wb_params = pd.DataFrame()\n",
    "all_pipeline_day_agg_params = pd.DataFrame()\n",
    "\n",
    "# get the HA healthy thresholds and rename them to \"CP\" so that we can use them to do some basic thresholding on the data\n",
    "ha_thresholds = get_mobilised_dmo_thresholds().xs(\"HA\", level=1, drop_level=False)\n",
    "new_index = pd.MultiIndex.from_tuples([(dmo, cohort) for dmo, _ in ha_thresholds.index for cohort in cohorts], names=ha_thresholds.index.names)\n",
    "duplicated_values = pd.concat([ha_thresholds] * len(cohorts), axis=0).reset_index(drop=True)\n",
    "multi_cohort_thresholds = pd.DataFrame(duplicated_values.values, index=new_index, columns=ha_thresholds.columns)\n",
    "\n",
    "for cohort in cohorts:\n",
    "    data = dataset.get_subset(cohort=cohort)\n",
    "    subjects = list({row[1] for row in dataset.group_labels if row[0] == cohort})\n",
    "    \n",
    "    for subject in subjects:\n",
    "        for day in [f\"Day {x}\" for x in range(1, 11)]:\n",
    "            try:\n",
    "                test = data.get_subset(Test=\"Test1\", day=day, subject_id=str(subject))\n",
    "            except KeyError as k:\n",
    "                msg = str(k)\n",
    "                if \"No datapoint in the dataset matched the following filter\" in msg:\n",
    "                    print(f\"{day} does not exist for subject {subject}\")\n",
    "                    continue\n",
    "                # raise unexpected key errors\n",
    "                raise\n",
    "                \n",
    "            try:\n",
    "                pipeline = MobilisedPipelineImpaired(dmo_thresholds=multi_cohort_thresholds)\n",
    "                pipeline = pipeline.safe_run(test)\n",
    "                meta = subject_metadata.loc[subject_metadata[\"Subject_ID\"] == int(subject)]\n",
    "                \n",
    "                day_wb_started = pd.to_timedelta(pipeline.per_wb_parameters_['start'], unit='ms') + test.data[\"LowerBack\"].index[0]\n",
    "                day_wb_started = day_wb_started.dt.day_name()\n",
    "                print(day_wb_started)\n",
    "                pipeline.per_wb_parameters_.insert(loc = 0, column = \"subject_id\", value = str(subject))\n",
    "                pipeline.per_wb_parameters_.insert(loc = 1, column = \"day\", value = day)\n",
    "                pipeline.per_wb_parameters_.insert(loc = 2, column = \"day_name\", value = day_wb_started)\n",
    "                pipeline.per_wb_parameters_.insert(loc = 3, column = \"cohort\", value = cohort)\n",
    "                pipeline.per_wb_parameters_.insert(loc = 4, column = \"height\", value = meta[\"Height\"].values[0])\n",
    "                pipeline.per_wb_parameters_.insert(loc = 5, column = \"weight\", value = meta[\"Weight\"].values[0])\n",
    "                pipeline.per_wb_parameters_.insert(loc = 6, column = \"age\", value = meta[\"Age\"].values[0])\n",
    "                pipeline.per_wb_parameters_.insert(loc = 7, column = \"gender\", value = meta[\"Gender\"].values[0])\n",
    "                pipeline.per_wb_parameters_.insert(loc = 8, column = \"handedness\", value = meta[\"Handedness\"].values[0])\n",
    "                # add time offset to wb parameters to get absolute timestamps\n",
    "                pipeline.per_wb_parameters_['start'] = pd.to_timedelta(pipeline.per_wb_parameters_['start']*10, unit='ms') + test.data[\"LowerBack\"].index[0]\n",
    "                pipeline.per_wb_parameters_['end'] = pd.to_timedelta(pipeline.per_wb_parameters_['end']*10, unit='ms') + test.data[\"LowerBack\"].index[0]\n",
    "\n",
    "                \n",
    "                pipeline.aggregated_parameters_.insert(loc = 0, column = \"subject_id\", value = str(subject))\n",
    "                pipeline.aggregated_parameters_.insert(loc = 1, column = \"day\", value = day)\n",
    "                pipeline.aggregated_parameters_.insert(loc = 2, column = \"day_name\", value = day_wb_started.values[0])\n",
    "                pipeline.aggregated_parameters_.insert(loc = 3, column = \"cohort\", value = cohort)\n",
    "                pipeline.aggregated_parameters_.insert(loc = 4, column = \"height\", value = meta[\"Height\"].values[0])\n",
    "                pipeline.aggregated_parameters_.insert(loc = 5, column = \"weight\", value = meta[\"Weight\"].values[0])\n",
    "                pipeline.aggregated_parameters_.insert(loc = 6, column = \"age\", value = meta[\"Age\"].values[0])\n",
    "                pipeline.aggregated_parameters_.insert(loc = 7, column = \"gender\", value = meta[\"Gender\"].values[0])\n",
    "                pipeline.aggregated_parameters_.insert(loc = 8, column = \"handedness\", value = meta[\"Handedness\"].values[0])\n",
    "                \n",
    "                all_pipeline_day_wb_params = pd.concat([all_pipeline_day_wb_params, pipeline.per_wb_parameters_]).reset_index(drop=True)\n",
    "                all_pipeline_day_agg_params = pd.concat([all_pipeline_day_agg_params, pipeline.aggregated_parameters_]).reset_index(drop=True)\n",
    "                \n",
    "            except ValueError as e:\n",
    "                msg = str(e)\n",
    "                if \"Expected sensor data for\" in msg and \"LowerBack\" in msg:\n",
    "                    print(f\"Skipping {day} — missing LowerBack sensor\")\n",
    "                    continue\n",
    "                # raise unexpected value errors\n",
    "                raise\n",
    "\n",
    "# drop columns we don't want\n",
    "all_pipeline_day_wb_params = all_pipeline_day_wb_params.drop([\"rule_name\", \"rule_obj\"], axis=1)\n",
    "\n",
    "all_pipeline_day_wb_params.to_csv(r\"Free-Living Parameters/per_wb_parameters.csv\")\n",
    "all_pipeline_day_agg_params.to_csv(r\"Free-Living Parameters/all_day_parameters.csv\")\n",
    "\n",
    "display(all_pipeline_day_wb_params)\n",
    "display(all_pipeline_day_agg_params)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "# histogram of durations\n",
    "axes[0].hist(all_pipeline_day_wb_params[\"duration_s\"], bins=100, edgecolor=\"black\")\n",
    "axes[0].set_title(\"Walking Bout Duration\")\n",
    "axes[0].set_xlabel(\"Duration (s)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# histogram of stride counts\n",
    "axes[1].hist(all_pipeline_day_wb_params[\"n_strides\"], bins=100, edgecolor=\"black\")\n",
    "axes[1].set_title(\"Number of Strides per Walking Bout\")\n",
    "axes[1].set_xlabel(\"Number of Strides per Walking Bout\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# histogram of stride counts\n",
    "axes[2].hist(all_pipeline_day_wb_params[\"stride_duration_s\"], bins=100, edgecolor=\"black\")\n",
    "axes[2].set_title(\"Stride Duration per Walking Bout\")\n",
    "axes[2].set_xlabel(\"Stride Duration (s)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "\n",
    "# histogram of stride counts\n",
    "axes[3].hist(all_pipeline_day_wb_params[\"cadence_spm\"], bins=100, edgecolor=\"black\")\n",
    "axes[3].set_title(\"Cadence per Walking Bout\")\n",
    "axes[3].set_xlabel(\"Cadence (steps/min)\")\n",
    "axes[3].set_ylabel(\"Count\")\n",
    "\n",
    "# histogram of stride counts\n",
    "axes[4].hist(all_pipeline_day_wb_params[\"stride_length_m\"], bins=100, edgecolor=\"black\")\n",
    "axes[4].set_title(\"Stride Length per Walking Bout\")\n",
    "axes[4].set_xlabel(\"Stride Length (m)\")\n",
    "axes[4].set_ylabel(\"Count\")\n",
    "\n",
    "# histogram of stride counts\n",
    "axes[5].hist(all_pipeline_day_wb_params[\"walking_speed_mps\"], bins=100, edgecolor=\"black\")\n",
    "axes[5].set_title(\"Walking Speed per Walking Bout\")\n",
    "axes[5].set_xlabel(\"Walking Speed (mps)\")\n",
    "axes[5].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e516d-9d25-4859-a6db-9b69abc18d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Existing aggregation ---\n",
    "df = all_pipeline_day_wb_params.copy(deep=True)\n",
    "df['start'] = pd.to_datetime(df['start'])\n",
    "df['hour_temp'] = df['start'].dt.floor('H')\n",
    "df['hour'] = df['hour_temp'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "sum_cols = ['n_strides', 'duration_s', 'n_raw_initial_contacts']\n",
    "mean_cols = ['duration_s', 'stride_duration_s', 'cadence_spm', 'stride_length_m', 'walking_speed_mps']\n",
    "\n",
    "agg_dict = {}\n",
    "for col in sum_cols:\n",
    "    agg_dict[f\"{col}_sum\"] = (col, 'sum')\n",
    "for col in mean_cols:\n",
    "    agg_dict[f\"{col}_mean\"] = (col, 'mean')\n",
    "\n",
    "grouped = (\n",
    "    df.groupby(['subject_id', 'day', 'day_name', 'cohort', 'hour'], as_index=False)\n",
    "      .agg(**agg_dict)\n",
    ")\n",
    "\n",
    "id_cols = ['subject_id', 'day', 'day_name', 'cohort', 'hour']\n",
    "metric_cols = [c for c in grouped.columns if c not in id_cols]\n",
    "grouped = grouped[id_cols + metric_cols]\n",
    "\n",
    "\n",
    "# --- STEP 1: Create a full 24-hour template for each subject/day ---\n",
    "hours_full = pd.date_range('00:00:00', '23:00:00', freq='H').strftime('%H:%M:%S')\n",
    "\n",
    "subjects_days = grouped[['subject_id', 'day', 'day_name', 'cohort']].drop_duplicates()\n",
    "\n",
    "template = (\n",
    "    subjects_days\n",
    "    .assign(key=1)\n",
    "    .merge(pd.DataFrame({'hour': hours_full, 'key': 1}), on='key')\n",
    "    .drop('key', axis=1)\n",
    ")\n",
    "\n",
    "# --- STEP 2: Merge your grouped data with the full template ---\n",
    "merged = pd.merge(\n",
    "    template,\n",
    "    grouped,\n",
    "    on=['subject_id', 'day', 'day_name', 'cohort', 'hour'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged = merged.astype({\n",
    "    'n_strides_sum': 'float64',\n",
    "    'duration_s_sum': 'float64',\n",
    "    'n_raw_initial_contacts_sum': 'float64'\n",
    "})\n",
    "\n",
    "# --- STEP 3: Optional reorder and display ---\n",
    "cols = ['subject_id', 'day', 'day_name', 'cohort', 'hour'] + metric_cols\n",
    "merged = merged[cols]\n",
    "\n",
    "display(merged.head(10))\n",
    "\n",
    "merged.to_csv(r\"Free-Living Parameters/hourly_grouped_parameters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f976c-3a95-482d-94f9-834fc1c88531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedaa9d9-f604-4ebe-9b70-482c2b1b1eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728d08d-c78c-448e-8a9a-50b83cb765b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do:\n",
    "# - create tools to plot graphs based on weight, height, age, gender, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38202182-4825-4649-9a68-c8a90b650543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
